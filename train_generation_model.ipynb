{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twUbX2MI13iO",
        "outputId": "45d66cca-1dd8-4296-bc3f-a925ddfe555f"
      },
      "outputs": [],
      "source": [
        "#!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import time\n",
        "import imageio\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from medmnist import DermaMNIST\n",
        "from medmnist.info import INFO\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DJOb4HqM1Oue"
      },
      "outputs": [],
      "source": [
        "def load_emoji(index, path=\"datasets/emoji/emoji.png\"):\n",
        "    im = imageio.imread(path)\n",
        "    emoji = np.array(im[:, index*40:(index+1)*40].astype(np.float32))\n",
        "    emoji /= 255.0\n",
        "    return emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TgFhyC521n9p"
      },
      "outputs": [],
      "source": [
        "class SamplePool:\n",
        "    def __init__(self, *, _parent=None, _parent_idx=None, **slots):\n",
        "        self._parent = _parent\n",
        "        self._parent_idx = _parent_idx\n",
        "        self._slot_names = slots.keys()\n",
        "        self._size = None\n",
        "        for k, v in slots.items():\n",
        "            if self._size is None:\n",
        "                self._size = len(v)\n",
        "            assert self._size == len(v)\n",
        "            setattr(self, k, np.asarray(v))\n",
        "\n",
        "    def sample(self, n):\n",
        "        idx = np.random.choice(self._size, n, False)\n",
        "        batch = {k: getattr(self, k)[idx] for k in self._slot_names}\n",
        "        batch = SamplePool(**batch, _parent=self, _parent_idx=idx)\n",
        "        return batch\n",
        "\n",
        "    def commit(self):\n",
        "        for k in self._slot_names:\n",
        "            getattr(self._parent, k)[self._parent_idx] = getattr(self, k)\n",
        "\n",
        "def make_seed(shape, n_channels):\n",
        "    seed = np.zeros([shape[0], shape[1], n_channels], np.float32)\n",
        "    seed[shape[0]//2, shape[1]//2, 3:] = 1.0\n",
        "    return seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CuOGkAHO1dbH"
      },
      "outputs": [],
      "source": [
        "class CAModel(nn.Module):\n",
        "    def __init__(self, channel_n, fire_rate, device, hidden_size=128):\n",
        "        super(CAModel, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.channel_n = channel_n\n",
        "\n",
        "        self.fc0 = nn.Linear(channel_n*3, hidden_size)\n",
        "        self.fc1 = nn.Linear(hidden_size, channel_n, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.fc1.weight.zero_()\n",
        "\n",
        "        self.fire_rate = fire_rate\n",
        "        self.to(self.device)\n",
        "\n",
        "    def alive(self, x):\n",
        "        return F.max_pool2d(x[:, 3:4, :, :], kernel_size=3, stride=1, padding=1) > 0.1\n",
        "\n",
        "    def perceive(self, x, angle):\n",
        "\n",
        "        def _perceive_with(x, weight):\n",
        "            conv_weights = torch.from_numpy(weight.astype(np.float32)).to(self.device)\n",
        "            conv_weights = conv_weights.view(1,1,3,3).repeat(self.channel_n, 1, 1, 1)\n",
        "            return F.conv2d(x, conv_weights, padding=1, groups=self.channel_n)\n",
        "\n",
        "        dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
        "        dy = dx.T\n",
        "        c = np.cos(angle*np.pi/180)\n",
        "        s = np.sin(angle*np.pi/180)\n",
        "        w1 = c*dx-s*dy\n",
        "        w2 = s*dx+c*dy\n",
        "\n",
        "        y1 = _perceive_with(x, w1)\n",
        "        y2 = _perceive_with(x, w2)\n",
        "        y = torch.cat((x,y1,y2),1)\n",
        "        return y\n",
        "\n",
        "    def update(self, x, fire_rate, angle):\n",
        "        x = x.transpose(1,3)\n",
        "        pre_life_mask = self.alive(x)\n",
        "\n",
        "        dx = self.perceive(x, angle)\n",
        "        dx = dx.transpose(1,3)\n",
        "        dx = self.fc0(dx)\n",
        "        dx = F.relu(dx)\n",
        "        dx = self.fc1(dx)\n",
        "\n",
        "        if fire_rate is None:\n",
        "            fire_rate=self.fire_rate\n",
        "        stochastic = torch.rand([dx.size(0),dx.size(1),dx.size(2),1])>fire_rate\n",
        "        stochastic = stochastic.float().to(self.device)\n",
        "        dx = dx * stochastic\n",
        "\n",
        "        x = x+dx.transpose(1,3)\n",
        "\n",
        "        post_life_mask = self.alive(x)\n",
        "        life_mask = (pre_life_mask & post_life_mask).float()\n",
        "        x = x * life_mask\n",
        "        return x.transpose(1,3)\n",
        "\n",
        "    def forward(self, x, steps=1, fire_rate=None, angle=0.0):\n",
        "        for step in range(steps):\n",
        "            x = self.update(x, fire_rate, angle)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTKTYCJM08KZ",
        "outputId": "04ceedec-f1c1-4449-e418-17048cbef3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: C:\\Users\\nicla\\.medmnist\\dermamnist.npz\n",
            "dataset length 779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 5/1000 [01:03<3:26:27, 12.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64 loss = 0.021328972652554512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 6/1000 [01:24<3:54:27, 14.15s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(seed[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], BATCH_SIZE, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    102\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x0\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 104\u001b[0m x, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_WANDB:\n\u001b[0;32m    107\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n",
            "Cell \u001b[1;32mIn[6], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x, target, steps, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(x, target, steps, optimizer, scheduler):\n\u001b[1;32m---> 80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(x[:, :, :, :\u001b[38;5;241m4\u001b[39m], target)\n\u001b[0;32m     82\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\nicla\\anaconda3\\envs\\dgm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicla\\anaconda3\\envs\\dgm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36mCAModel.forward\u001b[1;34m(self, x, steps, fire_rate, angle)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fire_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, angle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m---> 63\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfire_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mCAModel.update\u001b[1;34m(self, x, fire_rate, angle)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     40\u001b[0m pre_life_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malive(x)\n\u001b[1;32m---> 42\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m dx \u001b[38;5;241m=\u001b[39m dx\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     44\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc0(dx)\n",
            "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mCAModel.perceive\u001b[1;34m(self, x, angle)\u001b[0m\n\u001b[0;32m     30\u001b[0m w1 \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m*\u001b[39mdx\u001b[38;5;241m-\u001b[39ms\u001b[38;5;241m*\u001b[39mdy\n\u001b[0;32m     31\u001b[0m w2 \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m*\u001b[39mdx\u001b[38;5;241m+\u001b[39mc\u001b[38;5;241m*\u001b[39mdy\n\u001b[1;32m---> 33\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[43m_perceive_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m y2 \u001b[38;5;241m=\u001b[39m _perceive_with(x, w2)\n\u001b[0;32m     35\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x,y1,y2),\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mCAModel.perceive.<locals>._perceive_with\u001b[1;34m(x, weight)\u001b[0m\n\u001b[0;32m     22\u001b[0m conv_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(weight\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     23\u001b[0m conv_weights \u001b[38;5;241m=\u001b[39m conv_weights\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_n, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_n\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "USE_WANDB = False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"remaster_1.pth\"\n",
        "\n",
        "CHANNEL_N = 16        # Number of CA state channels\n",
        "TARGET_PADDING = 0    # 16   # Number of pixels used to pad the target image border\n",
        "TARGET_SIZE = 64\n",
        "\n",
        "lr = 2e-3\n",
        "lr_gamma = 0.9999\n",
        "betas = (0.5, 0.5)\n",
        "n_epoch = 1000\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "POOL_SIZE = 1024\n",
        "CELL_FIRE_RATE = 0.5\n",
        "\n",
        "EXPERIMENT_TYPE = \"Persistent\"\n",
        "EXPERIMENT_MAP = {\"Growing\":0, \"Persistent\":1, \"Regenerating\":2}\n",
        "EXPERIMENT_N = EXPERIMENT_MAP[EXPERIMENT_TYPE]\n",
        "\n",
        "USE_PATTERN_POOL = [0, 1, 1][EXPERIMENT_N]\n",
        "DERMAMNIST_CLASSES = INFO[\"dermamnist\"][\"label\"]\n",
        "\n",
        "dermaMnist_dataset = DermaMNIST(split=\"train\", download=True, as_rgb=True, size=64)\n",
        "\n",
        "melanoma_samples = []\n",
        "np.random.seed(seed=42)\n",
        "for i, sample in enumerate(dermaMnist_dataset):\n",
        "    if sample[1][0] == 4:\n",
        "        img_rgba = sample[0].convert(\"RGBA\")\n",
        "        melanoma_samples.append(np.array(img_rgba, dtype=np.float32) / 255)\n",
        "print(f\"dataset length {len(melanoma_samples)}\")\n",
        "\n",
        "np.random.shuffle(melanoma_samples)\n",
        "target_img = melanoma_samples[:100]\n",
        "\n",
        "# plt.figure(figsize=(4,4))\n",
        "# plt.imshow(target_img)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "p = TARGET_PADDING\n",
        "pad_target = np.pad(target_img, [(0,0), (p, p), (p, p), (0, 0)])\n",
        "h, w = pad_target.shape[1:3]\n",
        "# pad_target = np.expand_dims(pad_target, axis=0)\n",
        "pad_target = torch.from_numpy(pad_target.astype(np.float32)).to(device)\n",
        "\n",
        "train_dataloader = DataLoader(pad_target, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "seed = make_seed((h, w), CHANNEL_N)\n",
        "pool = SamplePool(x=np.repeat(seed[None, ...], POOL_SIZE, 0))\n",
        "batch = pool.sample(BATCH_SIZE).x\n",
        "\n",
        "ca = CAModel(CHANNEL_N, CELL_FIRE_RATE, device).to(device)\n",
        "# ca.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "optimizer = optim.Adam(ca.parameters(), lr=lr, betas=betas)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_gamma)\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    import secret\n",
        "\n",
        "    os.environ[\"WANDB_API_KEY\"] = secret.key\n",
        "    wandb.init(project=\"GrowingNCA\")\n",
        "    wandb.watch(ca, log='gradients', log_freq=BATCH_SIZE)\n",
        "    wandb.watch(ca, log='parameters', log_freq=BATCH_SIZE)\n",
        "\n",
        "loss_log = []\n",
        "\n",
        "def train(x, target, steps, optimizer, scheduler):\n",
        "    x = ca(x, steps=steps)\n",
        "    loss = F.mse_loss(x[:, :, :, :4], target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    return x, loss\n",
        "\n",
        "def loss_f(x, target):\n",
        "    return torch.mean(torch.pow(x[..., :4]-target, 2), [-2,-3,-1])\n",
        "\n",
        "for i in tqdm(range(1, n_epoch+1)):\n",
        "    for pad_target in train_dataloader:\n",
        "        if USE_PATTERN_POOL:\n",
        "            batch = pool.sample(pad_target.shape[0])\n",
        "            x0 = torch.from_numpy(batch.x.astype(np.float32)).to(device)\n",
        "            loss_rank = loss_f(x0, pad_target).detach().cpu().numpy().argsort()[::-1]\n",
        "            x0 = batch.x[loss_rank]\n",
        "            x0[:1] = seed\n",
        "\n",
        "        else:\n",
        "            x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
        "        x0 = torch.from_numpy(x0.astype(np.float32)).to(device)\n",
        "\n",
        "        x, loss = train(x0, pad_target, np.random.randint(64,96), optimizer, scheduler)\n",
        "\n",
        "        if USE_WANDB:\n",
        "            wandb.log({'model_loss': loss.item()})\n",
        "\n",
        "        if USE_PATTERN_POOL:\n",
        "            batch.x[:] = x.detach().cpu().numpy()\n",
        "            batch.commit()\n",
        "\n",
        "        step_i = len(loss_log)\n",
        "        loss_log.append(loss.item())\n",
        "\n",
        "    if i%5 == 0:  # step_i%100 == 0:\n",
        "        # clear_output()\n",
        "        print(step_i, \"loss =\", loss.item())\n",
        "        # visualize_batch(x0.detach().cpu().numpy(), x.detach().cpu().numpy())\n",
        "        # plot_loss(loss_log)\n",
        "        torch.save(ca.state_dict(), model_path + f\"_{i}\")\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
