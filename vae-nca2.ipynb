{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from medmnist import DermaMNIST\n",
    "from medmnist.info import INFO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Niclas\\.medmnist\\dermamnist_64.npz\n",
      "dataset length 779\n"
     ]
    }
   ],
   "source": [
    "dermaMnist_dataset = DermaMNIST(split=\"train\", download=True, as_rgb=True, size=64)\n",
    "\n",
    "melanoma_samples = []\n",
    "np.random.seed(seed=42)\n",
    "for i, sample in enumerate(dermaMnist_dataset):\n",
    "    if sample[1][0] == 4:\n",
    "        img_rgba = sample[0].convert(\"RGBA\")\n",
    "        melanoma_samples.append(np.array(img_rgba, dtype=np.float32) / 255)\n",
    "print(f\"dataset length {len(melanoma_samples)}\")\n",
    "\n",
    "np.random.shuffle(melanoma_samples)\n",
    "target_img = melanoma_samples\n",
    "\n",
    "initial_images = torch.rand(len(melanoma_samples), 64, 64, 16)\n",
    "\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.imshow(target_img)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "p = 0\n",
    "pad_target = np.pad(target_img, [(0,0), (p, p), (p, p), (0, 0)])\n",
    "h, w = pad_target.shape[1:3]\n",
    "# pad_target = np.expand_dims(pad_target, axis=0)\n",
    "pad_target = torch.from_numpy(pad_target.astype(np.float32)).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(pad_target, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, latent_dim)  # Changed from 64 * 8 * 8 to 64 * 16 * 16\n",
    "        self.fc2 = nn.Linear(64 * 16 * 16, latent_dim)  # Changed from 64 * 8 * 8 to 64 * 16 * 16\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # Change shape from (batch_size, height, width, channels) to (batch_size, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
    "        mean = self.fc1(x)\n",
    "        log_var = self.fc2(x)\n",
    "        return mean, log_var\n",
    "\n",
    "# Reparameterization Trick\n",
    "def reparameterize(mean, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mean + eps * std\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 16 * 16)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, output_channels, 4, 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 64, 16, 16)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# Define the NCA Model (same as before, can add more complexity if needed)\n",
    "class CAModel(nn.Module):\n",
    "    def __init__(self, channel_n, fire_rate, device, hidden_size=128):\n",
    "        super(CAModel, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.channel_n = channel_n\n",
    "\n",
    "        self.fc0 = nn.Linear(channel_n*3, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, channel_n, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.zero_()\n",
    "\n",
    "        self.fire_rate = fire_rate\n",
    "        self.to(self.device)\n",
    "\n",
    "    def alive(self, x):\n",
    "        return F.max_pool2d(x[:, 3:4, :, :], kernel_size=3, stride=1, padding=1) > 0.1\n",
    "\n",
    "    def perceive(self, x, angle):\n",
    "\n",
    "        def _perceive_with(x, weight):\n",
    "            conv_weights = torch.from_numpy(weight.astype(np.float32)).to(self.device)\n",
    "            conv_weights = conv_weights.view(1,1,3,3).repeat(self.channel_n, 1, 1, 1)\n",
    "            return F.conv2d(x, conv_weights, padding=1, groups=self.channel_n)\n",
    "\n",
    "        dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
    "        dy = dx.T\n",
    "        c = np.cos(angle*np.pi/180)\n",
    "        s = np.sin(angle*np.pi/180)\n",
    "        w1 = c*dx-s*dy\n",
    "        w2 = s*dx+c*dy\n",
    "\n",
    "        y1 = _perceive_with(x, w1)\n",
    "        y2 = _perceive_with(x, w2)\n",
    "        y = torch.cat((x,y1,y2),1)\n",
    "        return y\n",
    "\n",
    "    def update(self, x, fire_rate, angle):\n",
    "        x = x.transpose(1,3)\n",
    "        pre_life_mask = self.alive(x)\n",
    "\n",
    "        dx = self.perceive(x, angle)\n",
    "        dx = dx.transpose(1,3)\n",
    "        dx = self.fc0(dx)\n",
    "        dx = F.relu(dx)\n",
    "        dx = self.fc1(dx)\n",
    "\n",
    "        if fire_rate is None:\n",
    "            fire_rate=self.fire_rate\n",
    "        stochastic = torch.rand([dx.size(0),dx.size(1),dx.size(2),1])>fire_rate\n",
    "        stochastic = stochastic.float().to(self.device)\n",
    "        dx = dx * stochastic\n",
    "\n",
    "        x = x+dx.transpose(1,3)\n",
    "\n",
    "        post_life_mask = self.alive(x)\n",
    "        life_mask = (pre_life_mask & post_life_mask).float()\n",
    "        x = x * life_mask\n",
    "        return x.transpose(1,3)\n",
    "\n",
    "    def forward(self, x, steps=1, fire_rate=None, angle=0.0):\n",
    "        for step in range(steps):\n",
    "            x = self.update(x, fire_rate, angle)\n",
    "        return x\n",
    "\n",
    "# VAE-NCA Combined Model\n",
    "class VAE_NCA(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim, nca_channels, fire_rate, device):\n",
    "        super(VAE_NCA, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim).to(device)\n",
    "        self.decoder = Decoder(latent_dim, input_channels).to(device)\n",
    "        self.nca = CAModel(nca_channels, fire_rate, device).to(device)\n",
    "\n",
    "    def forward(self, x, nca_steps=1):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = reparameterize(mean, log_var)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        x_reconstructed = torch.cat((x_reconstructed, torch.ones((x_reconstructed.shape[0], 12, 64, 64)).to(device)), dim=1).permute(0, 2, 3, 1)\n",
    "        x_nca = self.nca(x_reconstructed, steps=nca_steps)\n",
    "        \n",
    "        return x_nca, x_reconstructed, mean, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1467.4912109375\n",
      "Epoch 1, Loss: 1018.882080078125\n",
      "Epoch 2, Loss: 799.9459228515625\n",
      "Epoch 3, Loss: 755.3085327148438\n",
      "Epoch 4, Loss: 680.0865478515625\n",
      "Epoch 5, Loss: 764.510498046875\n",
      "Epoch 6, Loss: 687.4827880859375\n",
      "Epoch 7, Loss: 694.9779052734375\n",
      "Epoch 8, Loss: 615.7159423828125\n",
      "Epoch 9, Loss: 627.021484375\n",
      "Epoch 10, Loss: 542.8441162109375\n",
      "Epoch 11, Loss: 590.9852905273438\n",
      "Epoch 12, Loss: 590.8919677734375\n",
      "Epoch 13, Loss: 560.4330444335938\n",
      "Epoch 14, Loss: 563.6632690429688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m x_nca, x_reconstructed, mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mvae_nca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnca_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(x_nca[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m4\u001b[39m], x, x_reconstructed[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m4\u001b[39m], mean, log_var)\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Niclas\\anaconda3\\envs\\dgm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Niclas\\anaconda3\\envs\\dgm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 117\u001b[0m, in \u001b[0;36mVAE_NCA.forward\u001b[1;34m(self, x, nca_steps)\u001b[0m\n\u001b[0;32m    115\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m    116\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_reconstructed, torch\u001b[38;5;241m.\u001b[39mones((x_reconstructed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m x_nca \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reconstructed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnca_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_nca, x_reconstructed, mean, log_var\n",
      "File \u001b[1;32mc:\\Users\\Niclas\\anaconda3\\envs\\dgm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Niclas\\anaconda3\\envs\\dgm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 101\u001b[0m, in \u001b[0;36mCAModel.forward\u001b[1;34m(self, x, steps, fire_rate, angle)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fire_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, angle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m--> 101\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfire_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[31], line 80\u001b[0m, in \u001b[0;36mCAModel.update\u001b[1;34m(self, x, fire_rate, angle)\u001b[0m\n\u001b[0;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     78\u001b[0m pre_life_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malive(x)\n\u001b[1;32m---> 80\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m dx \u001b[38;5;241m=\u001b[39m dx\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     82\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc0(dx)\n",
      "Cell \u001b[1;32mIn[31], line 71\u001b[0m, in \u001b[0;36mCAModel.perceive\u001b[1;34m(self, x, angle)\u001b[0m\n\u001b[0;32m     68\u001b[0m w1 \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m*\u001b[39mdx\u001b[38;5;241m-\u001b[39ms\u001b[38;5;241m*\u001b[39mdy\n\u001b[0;32m     69\u001b[0m w2 \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m*\u001b[39mdx\u001b[38;5;241m+\u001b[39mc\u001b[38;5;241m*\u001b[39mdy\n\u001b[1;32m---> 71\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[43m_perceive_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m y2 \u001b[38;5;241m=\u001b[39m _perceive_with(x, w2)\n\u001b[0;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x,y1,y2),\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 60\u001b[0m, in \u001b[0;36mCAModel.perceive.<locals>._perceive_with\u001b[1;34m(x, weight)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perceive_with\u001b[39m(x, weight):\n\u001b[1;32m---> 60\u001b[0m     conv_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     conv_weights \u001b[38;5;241m=\u001b[39m conv_weights\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_n, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(x, conv_weights, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss Function\n",
    "def loss_function(x_nca, x, x_reconstructed, mean, log_var):\n",
    "    recon_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    nca_loss = F.mse_loss(x_nca, x, reduction='sum')\n",
    "    return recon_loss + kl_div + nca_loss\n",
    "\n",
    "# Example Usage\n",
    "input_channels = 4\n",
    "latent_dim = 128\n",
    "nca_channels = 16\n",
    "fire_rate = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vae_nca = VAE_NCA(input_channels, latent_dim, nca_channels, fire_rate, device)\n",
    "optimizer = optim.Adam(vae_nca.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    for x in train_dataloader:  # Assuming train_dataloader is defined\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_nca, x_reconstructed, mean, log_var = vae_nca(x, nca_steps=20)\n",
    "        loss = loss_function(x_nca[..., :4], x, x_reconstructed[..., :4], mean, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save(vae_nca.state_dict(), f\"vae_model_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, latent_dim)  # Changed from 64 * 8 * 8 to 64 * 16 * 16\n",
    "        self.fc2 = nn.Linear(64 * 16 * 16, latent_dim)  # Changed from 64 * 8 * 8 to 64 * 16 * 16\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # Change shape from (batch_size, height, width, channels) to (batch_size, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
    "        mean = self.fc1(x)\n",
    "        log_var = self.fc2(x)\n",
    "        return mean, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 16 * 16)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
    "        self.conv2 = nn.ConvTranspose2d(32, output_channels, 4, 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 64, 16, 16)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_channels = 4\n",
    "latent_dim = 128\n",
    "encoder = Encoder(input_channels, latent_dim)\n",
    "decoder = Decoder(latent_dim, input_channels)\n",
    "\n",
    "# Create a dummy input with shape (8, 64, 64, 4)\n",
    "x = torch.randn(8, 64, 64, 4).to(device)\n",
    "mean, log_var = encoder(x)\n",
    "z = torch.randn_like(mean)  # Normally you would use the reparameterization trick\n",
    "x_reconstructed = decoder(z)\n",
    "print(\"Mean shape:\", mean.shape)\n",
    "print(\"Log variance shape:\", log_var.shape)\n",
    "print(\"Reconstructed shape:\", x_reconstructed.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
