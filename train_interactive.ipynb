{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from data.data import load_emoji\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from model.CAModel import CAModel\n",
    "from lib.utils_vis import SamplePool, get_living_mask, make_seed, make_circle_masks\n",
    "from utils.visualize import plot_loss, visualize_batch, to_alpha, to_rgb\n",
    "\n",
    "USE_WANDB = False\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"model/checkpoints/remaster_1.pth\"\n",
    "\n",
    "CHANNEL_N = 16        # Number of CA state channels\n",
    "TARGET_PADDING = 16   # Number of pixels used to pad the target image border\n",
    "TARGET_SIZE = 40\n",
    "\n",
    "lr = 2e-3\n",
    "lr_gamma = 0.9999\n",
    "betas = (0.5, 0.5)\n",
    "n_epoch = 80000\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "POOL_SIZE = 1024\n",
    "CELL_FIRE_RATE = 0.5\n",
    "\n",
    "TARGET_EMOJI = 0 #@param \"ðŸ¦Ž\"\n",
    "\n",
    "EXPERIMENT_TYPE = \"Regenerating\"\n",
    "EXPERIMENT_MAP = {\"Growing\":0, \"Persistent\":1, \"Regenerating\":2}\n",
    "EXPERIMENT_N = EXPERIMENT_MAP[EXPERIMENT_TYPE]\n",
    "\n",
    "USE_PATTERN_POOL = [0, 1, 1][EXPERIMENT_N]\n",
    "DAMAGE_N = [0, 0, 3][EXPERIMENT_N]  # Number of patterns to damage in a batch\n",
    "\n",
    "target_img = load_emoji(TARGET_EMOJI)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(to_rgb(target_img))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "p = TARGET_PADDING\n",
    "pad_target = np.pad(target_img, [(p, p), (p, p), (0, 0)])\n",
    "h, w = pad_target.shape[:2]\n",
    "pad_target = np.expand_dims(pad_target, axis=0)\n",
    "pad_target = torch.from_numpy(pad_target.astype(np.float32)).to(device)\n",
    "\n",
    "seed = make_seed((h, w), CHANNEL_N)\n",
    "pool = SamplePool(x=np.repeat(seed[None, ...], POOL_SIZE, 0))\n",
    "batch = pool.sample(BATCH_SIZE).x\n",
    "\n",
    "ca = CAModel(CHANNEL_N, CELL_FIRE_RATE, device).to(device)\n",
    "ca.load_state_dict(torch.load(model_path))\n",
    "\n",
    "optimizer = optim.Adam(ca.parameters(), lr=lr, betas=betas)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    import secret\n",
    "\n",
    "    os.environ[\"WANDB_API_KEY\"] = secret.key\n",
    "    wandb.init(project=\"GrowingNCA\")\n",
    "    wandb.watch(ca, log='gradients', log_freq=BATCH_SIZE)\n",
    "    wandb.watch(ca, log='parameters', log_freq=BATCH_SIZE)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "def train(x, target, steps, optimizer, scheduler):\n",
    "    x = ca(x, steps=steps)\n",
    "    loss = F.mse_loss(x[:, :, :, :4], target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return x, loss\n",
    "\n",
    "def loss_f(x, target):\n",
    "    return torch.mean(torch.pow(x[..., :4]-target, 2), [-2,-3,-1])\n",
    "\n",
    "for i in range(n_epoch+1):\n",
    "    if USE_PATTERN_POOL:\n",
    "        batch = pool.sample(BATCH_SIZE)\n",
    "        x0 = torch.from_numpy(batch.x.astype(np.float32)).to(device)\n",
    "        loss_rank = loss_f(x0, pad_target).detach().cpu().numpy().argsort()[::-1]\n",
    "        x0 = batch.x[loss_rank]\n",
    "        x0[:1] = seed\n",
    "        if DAMAGE_N:\n",
    "            damage = 1.0-make_circle_masks(DAMAGE_N, h, w)[..., None]\n",
    "            x0[-DAMAGE_N:] *= damage\n",
    "    else:\n",
    "        x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
    "    x0 = torch.from_numpy(x0.astype(np.float32)).to(device)\n",
    "\n",
    "    x, loss = train(x0, pad_target, np.random.randint(64,96), optimizer, scheduler)\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.log({'model_loss': loss.item()})\n",
    "    \n",
    "    if USE_PATTERN_POOL:\n",
    "        batch.x[:] = x.detach().cpu().numpy()\n",
    "        batch.commit()\n",
    "\n",
    "    step_i = len(loss_log)\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "    if step_i%100 == 0:\n",
    "        # clear_output()\n",
    "        print(step_i, \"loss =\", loss.item())\n",
    "        # visualize_batch(x0.detach().cpu().numpy(), x.detach().cpu().numpy())\n",
    "        # plot_loss(loss_log)\n",
    "        torch.save(ca.state_dict(), model_path)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
